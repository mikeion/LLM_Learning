{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "### Langchain Libraries\n",
    "\n",
    "### models\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "#### prompts\n",
    "from langchain import PromptTemplate\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import DeepLake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embeddings are a way to represent data as points in a high-dimensional space, where the distance and direction between points correspond to semantic or structural relationships. The most common use of embeddings is for words or phrases in NLP, but the concept is broadly applicable to any type of data that has relationships that can be represented spatially.\n",
    "\n",
    "For instance, word embeddings, like Word2Vec and GloVe, represent words as high-dimensional vectors (usually hundreds of dimensions) such that the vectorial distances between words correspond to their semantic or syntactic similarity. In such a representation, words with similar meanings are closer in the embedding space, and the direction between words can be used to infer relationships. For example, the famous Word2Vec analogy, \"King - Man + Woman = Queen,\" captures the gender relationship through vector arithmetic in the word embedding space.\n",
    "\n",
    "The reasons embeddings are useful for building AI applications are manifold:\n",
    "1. Data compression: Embeddings can be used to compress large amounts of data into a smaller space, while preserving the relationships between data points. This is useful for reducing the amount of data that needs to be stored or processed.\n",
    "2. Semantic Relationships: Embeddings represent semantic relationships between entities. This allows ML models to learn from the relationships between data points, rather than just the data points themselves. This is useful for building models that can generalize to new data, and for building models that can learn from small amounts of data.\n",
    "3. Transfer Learning: Pretrained embeddings learned on a large corpus can be transferred to other tasks with smaller amounts of data, leveraging the linguistic or other types of knowledge encoded in the embeddings.\n",
    "4. Interpretability: Embeddings can be used to visualize data in a way that is interpretable to humans. This is useful for understanding the relationships between data points, and for debugging ML models.\n",
    "5. Reduced noise: High dimensional data often contains a lot of noise or redundant information. By reducing  the noise in the data, embeddings can improve the performance of ML models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Lake\n",
    "\n",
    "Deep Lake is a Multi-Modal Vector Store that stores embeddings and their metadata including text, jsons, images, audio, video, and more. It can be used with Langchain to build apps that require vector filtering and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "\n",
    "import deeplake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/qilindage/langchain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://qilindage/langchain loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "ds = deeplake.load('hub://qilindage/langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
