{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental component of LangChain involves invoking an LLM with a specific input. To illustrate this, we'll explore a simple example. Let's imagine we are building a service that suggests personalized workout routines based on an individual's fitness goals and preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature parameter in OpenAI models manages the randomness of the output. When set to 0, the output is mostly predetermined and suitable for tasks requiring stability and the most probable result. At a setting of 1.0, the output can be inconsistent and interesting but isn't generally advised for most tasks. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity. The best setting should be determined by experimenting with different values for each specific use case. The code initializes the GPT-3 model’s Davinci variant. We will learn more about the various models and their differences later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will generate a personalized workout routine based on the user's fitness goals and preferences using the LLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Given a function f(x), graph its derivative f'(x).\n",
      "2. Find the derivative of (x^2*cos(x))/ln(x).\n",
      "3. Solve the chain rule for the derivative of h(x) = (x^2+1)^3.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "You are an MIT Professor of Mathematics teaching Calculus I. \\\n",
    "Suggest a set of three warm-up exercise for someone who just learned about the chain rules for derivatives \\\n",
    "1. You can assume the student knows how to take the derivative of logarithmic and trigonometic functions.\n",
    "2. You can assume the student knows how to use the power rule and the product rule.\n",
    "3. You can assume the student knows how to take the derivative of exponential functions.\n",
    "Please format your response in a numbered list, with each item on a new line.\n",
    "\"\"\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chains\n",
    "\n",
    "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the `LLMChain`, which consists of a `PromptTemplate`, a model (either an LLM or a ChatModel), and an optional output parser.\n",
    "\n",
    "The `LLMChain` works as follows:\n",
    "\n",
    "1. Takes (multiple) input variables.\n",
    "2. Uses the `PromptTemplate` to format the input variables into a prompt.\n",
    "3. Passes the formatted prompt to the model (LLM or ChatModel).\n",
    "4. If an output parser is provided, it uses the `OutputParser` to parse the output of the LLM into a final format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Calculate the derivative of the following function f(x) = (x^2 + 3x + 4)^4 using the chain rule.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=\"What is a good problem for a university Calculus student that tests their understanding of the {concept}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"chain rule for derivatives\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can change out the input variable and use different concepts to create different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A good problem to test a student's understanding of the limit definition of the derivative would be to ask them to find the derivative of the function f(x) = x^2 + 2x using the limit definition of the derivative.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"limit definition of the derivative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Problem:\n",
      "Given a function f(x) = x^2, what is the instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4]?\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Instantaneous rate of change of a function over an interval, where the derivative is thought of \\\n",
    "                 verbally as a rate of change. The problem should not deal with speeds or velocities and should \\\n",
    "                solely deal with rates of change of functions.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example showcases the flexibility and ease of using LangChain to create custom chains for various language generation tasks, including ones involving mathematics tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Memory\n",
    "\n",
    "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses. Memory, such as `ConversationBufferMemory`, acts as a wrapper around `ChatMessageHistory`, extracting the messages and providing them to the chain for better context-aware generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: I would like you to help me with some mathematics problems.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I would like you to help me with some mathematics problems.\n",
      "AI:  Sure, I'd be happy to help you with your math problems. What kind of math problems do you need help with?\n",
      "Human: Given a function f(x) = x^2, what is the instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4]?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: I would like you to help me with some mathematics problems.\n",
      "AI:  Sure, I'd be happy to help you with your math problems. What kind of math problems do you need help with?\n",
      "Human: Given a function f(x) = x^2, what is the instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4]?\n",
      "AI:  The instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4] is 4. This is because the midpoint of the interval is x = 1, and the derivative of f(x) = x^2 is f'(x) = 2x. Therefore, f'(1) = 2(1) = 4.\n",
      "Human: Can you give me detailed reasoning behind your answer to the last question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='I would like you to help me with some mathematics problems.', additional_kwargs={}, example=False), AIMessage(content=\" Sure, I'd be happy to help you with your math problems. What kind of math problems do you need help with?\", additional_kwargs={}, example=False), HumanMessage(content='Given a function f(x) = x^2, what is the instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4]?', additional_kwargs={}, example=False), AIMessage(content=\" The instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4] is 4. This is because the midpoint of the interval is x = 1, and the derivative of f(x) = x^2 is f'(x) = 2x. Therefore, f'(1) = 2(1) = 4.\", additional_kwargs={}, example=False), HumanMessage(content='Can you give me detailed reasoning behind your answer to the last question?', additional_kwargs={}, example=False), AIMessage(content=\" Sure. The derivative of a function f(x) is a measure of the rate of change of the function with respect to x. The derivative of f(x) = x^2 is f'(x) = 2x. This means that the rate of change of f(x) is twice the value of x. Therefore, when x = 1, the rate of change of f(x) is 2(1) = 4.\", additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history') callbacks=None callback_manager=None verbose=True prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True) llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_api_base=None, openai_organization=None, openai_proxy=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all') output_key='response' input_key='input'\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "conversation.predict(input=\"I would like you to help me with some mathematics problems.\")\n",
    "\n",
    "# Continue the conversation\n",
    "conversation.predict(input=\"Given a function f(x) = x^2, what is the instantaneous rate of change of f(x) at the midpoint of the interval [-2, 4]?\")\n",
    "conversation.predict(input=\"Can you give me detailed reasoning behind your answer to the last question?\")\n",
    "\n",
    "# Display the conversation\n",
    "print(conversation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Lake VectorStore\n",
    "\n",
    "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain, facilitating the development and deployment of applications.\n",
    "\n",
    "Deep Lake provides several advantages over the typical vector store:\n",
    "- It's multimodal, meaning it can store embeddings of different types, such as text, images, and audio.\n",
    "- It's serverless, which means that we can create and manage cloud datasets without creating and managing a database instance.\n",
    "- Lastly, it's possible to easily create a dataloader out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning ML models using common frameworks like Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/mikeion/langchain_course_custom_tool\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://mikeion/langchain_course_custom_tool loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://mikeion/langchain_course_custom_tool already exists, loading from the storage\n",
      "Dataset(path='hub://mikeion/langchain_course_custom_tool', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype    shape    dtype  compression\n",
      "  -------   -------  -------  -------  ------- \n",
      " embedding  generic   (0,)    float32   None   \n",
      "    ids      text     (0,)      str     None   \n",
      " metadata    json     (0,)      str     None   \n",
      "   text      text     (0,)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 1/1 [00:11<00:00\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://mikeion/langchain_course_custom_tool', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (3, 1536)  float32   None   \n",
      "    ids      text     (3, 1)      str     None   \n",
      " metadata    json     (3, 1)      str     None   \n",
      "   text      text     (3, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['e7a9993e-0f97-11ee-951e-e00af64ce908',\n",
       " 'e7a9993f-0f97-11ee-a1d8-e00af64ce908',\n",
       " 'e7a99940-0f97-11ee-9724-e00af64ce908']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import tiktoken\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
    "\n",
    "# instantiate the LLM and embeddings models\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create our documents\n",
    "texts = [\n",
    "\"Given a function f(x), graph its derivative f'(x)\",\n",
    "\"Find the derivative of (x^2*cos(x))/ln(x)\", \n",
    "\"Find the derivative of h(x) = (x^2+1)^3 using the chain rule\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# create Deep Lake dataset\n",
    "my_activeloop_org_id = \"mikeion\" \n",
    "my_activeloop_dataset_name = \"langchain_course_custom_tool\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a RetrievalQA chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "\tllm=llm,\n",
    "\tchain_type=\"stuff\",\n",
    "\tretriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "\ttools,\n",
    "\tllm,\n",
    "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find a derivative problem that involves the Chain rule.\n",
      "Action: Retrieval QA System\n",
      "Action Input: Derivative problem involving Chain rule\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m The derivative of h(x) = (x^2+1)^3 using the chain rule is 6(x^2+1)^2*2x.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The derivative of h(x) = (x^2+1)^3 using the chain rule is 6(x^2+1)^2*2x.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The derivative of h(x) = (x^2+1)^3 using the chain rule is 6(x^2+1)^2*2x.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"What's a good derivative problem for a Calculus student involving the Chain rule?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use Deep Lake as a vector database and create an agent with a `RetrievalQA` chain as a tool to answer questions based on the given document.\n",
    "\n",
    "Let’s aadd an example of reloading an existing vector store and adding more data.\n",
    "\n",
    "We first reload an existing vector store from Deep Lake that' located at a specified dataset path. Notice that this time we don't need to specify which embedding model to use again. Then, we load new textual data and split it into manageable chunks. Finally, we add these chunks to the existing dataset, creating and storing corresponding embeddings for each added text segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/mikeion/langchain_course_custom_tool\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://mikeion/langchain_course_custom_tool loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://mikeion/langchain_course_custom_tool already exists, loading from the storage\n",
      "Dataset(path='hub://mikeion/langchain_course_custom_tool', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (3, 1536)  float32   None   \n",
      "    ids      text     (3, 1)      str     None   \n",
      " metadata    json     (3, 1)      str     None   \n",
      "   text      text     (3, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 1/1 [00:08<00:00\n",
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://mikeion/langchain_course_custom_tool', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype     shape     dtype  compression\n",
      "  -------   -------   -------   -------  ------- \n",
      " embedding  generic  (5, 1536)  float32   None   \n",
      "    ids      text     (5, 1)      str     None   \n",
      " metadata    json     (5, 1)      str     None   \n",
      "   text      text     (5, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7d85d45f-0f99-11ee-a229-e00af64ce908',\n",
       " '7d85d460-0f99-11ee-a3da-e00af64ce908']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the existing Deep Lake dataset\n",
    "my_activeloop_org_id = \"mikeion\" # TODO: use your organization id here\n",
    "my_activeloop_dataset_name = \"langchain_course_custom_tool\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path)\n",
    "\n",
    "# Put some more calculus problems in the dataset.\n",
    "texts = [\n",
    "    \"Find the derivative of the function f(x) = x^2 + 2x using the limit definition of the derivative.\",\n",
    "    \"Find the derivative of the function f(x) = 5x^2 + 10x using the power rule.\",\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find a problem that involves the limit definition of the derivative\n",
      "Action: Retrieval QA System\n",
      "Action Input: Find a derivative problem involving the limit definition of the derivative\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "The limit definition of the derivative of a function f(x) at a point x = a is given by:\n",
      "lim h→0 (f(a+h) - f(a))/h\n",
      "\n",
      "So, a derivative problem involving the limit definition of the derivative would be to find the derivative of a function f(x) at a point x = a using the limit definition.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Find the derivative of a function f(x) at a point x = a using the limit definition.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Find the derivative of a function f(x) at a point x = a using the limit definition.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"What's a good derivative problem for a Calculus student involving the limit definition of the derivative\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents in LangChain\n",
    "\n",
    "In LangChain, agents are high-level components that use language models to determine with actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specifc duties, such as Google Search, database lookups, or Python REPL.\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that action, seeing an observation, and repeating that until done.\n",
    "\n",
    "Several types of agents are available in LangChain, including:\n",
    "- The `zero-shot-react-description` agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
    "- The `react-docstore` agent engages with a docstore through the ReAct framework. It needs two tools:\n",
    "  - A search tool: This tool finds a document\n",
    "  - A lookup tool: This tool searches for a term in the most recently found document.\n",
    "- The `self-ask-with-search` agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original selfask with the search paper, where a Google search API was provided as the tool.\n",
    "- The `conversational-react-description` agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions.\n",
    "\n",
    "In our next example, the Agent will use the Google Search tool to look up definitions about mathematics concepts and will generate responses based on this information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you want to set the environment variables `GOOGLE_API_KEY` and `GOOGLE_CSE_ID` to be able to use Google Search via API. Refer to this article for a guide on how to get them.\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/tools/integrations/google_search\n",
    "\n",
    "Then, let’s import the necessary modules:\n",
    "\n",
    "- `langchain.llms.OpenAI`: This is used to create an instance of the OpenAI language model, which can generate human-like text based on the input it's given.\n",
    "- `langchain.agents.load_tools`: This function is used to load a list of tools that an AI agent can use.\n",
    "- `langchain.agents.initialize_agent`: This function initializes an AI agent that can use a given set of tools and a language model to interact with users.\n",
    "- `langchain.agents.Tool`: This is a class used to define a tool that an AI agent can use. A tool is defined by its name, a function that performs the tool's action, and a description of the tool.\n",
    "- `langchain.utilities.GoogleSearchAPIWrapper`: This class is a wrapper for the Google Search API, allowing it to be used as a tool by an AI agent. It likely contains a method that sends a search query to Google and retrieves the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM and set the temperature to 0 for a more precise answer. \n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Define the Google Search Wrapper\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tool object represents a specific capability or function the system can use. In this case, it's a tool for performing Google searches.\n",
    "\n",
    "It is initialized with three parameters:\n",
    "- `name`: This is a string that serves as a unique identifier for the tool. In this case, the name of the tool is \"google-search.”\n",
    "- `func`: This parameter is assigned the function that the tool will execute when called. In this case, it's the `run` method of the `search` object, which presumably performs a Google search.\n",
    "- `description`: This is a string that briefly explains what the tool does. The description explains that this tool is helpful when you need to use Google to answer questions about current events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"google-search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an agent that uses our Google Search tool:\n",
    "- `initialize_agent()`: This function call creates and initializes an agent. An agent is a component that determines which actions to take based on user input. These actions can be using a tool, returning a response to the user, or something else.\n",
    "- `tools`:  represents the list of Tool objects that the agent can use.\n",
    "- `agent=\"zero-shot-react-description\"`: The \"zero-shot-react-description\" type of an Agent uses the ReAct framework to decide which tool to use based only on the tool's description.\n",
    "- `verbose=True`: when set to True, it will cause the Agent to print more detailed information about what it's doing. This is useful for debugging and understanding what's happening under the hood.\n",
    "- `max_iterations=6`: sets a limit on the number of iterations the Agent can perform before stopping. It's a way of preventing the agent from running indefinitely in some cases, which may have unwanted monetary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         max_iterations=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the definition of the chain rule of the derivative.\n",
      "Action: google-search\n",
      "Action Input: \"chain rule of the derivative definition\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIn calculus, the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the ... The chain rule tells us how to find the derivative of a composite function. Brush up on your knowledge of composite functions, and learn how to apply the ... Chain Rule And Composite Functions. In differential calculus, the chain rule is a formula used to find the derivative of a composite function. If ... The chain rule provides us a technique for finding the derivative of composite functions, with the number of functions that make up the composition ... Nov 16, 2022 ... Chain Rule · If we define F(x)=(f∘g)(x) F ( x ) = ( f ∘ g ) ( x ) then the derivative of F(x) F ( x ) is, F′(x)=f′(g(x))g′(x) F ′ ( x ) = f ′ ( ... Definition. • In calculus, the chain rule is a formula for computing the derivative of the composition of two or more functions. That. Aug 28, 2007 ... The chain rule can be thought of as taking the derivative of the outer function (applied to the inner function) and multiplying it times the ... In such cases, we apply the chain rule formula which says d/dx ( f(g(x) ) = f' (g(x)) · g' (x). Using this ... Examples · Example: What is the derivative of x2+x3 ? The Sum Rule says: the derivative of f + g = f' + g' · Example: What is ddv(v3−v4) ? The Difference Rule ... May 4, 2023 ... Chain rule says that the instantaneous rate of change of a function “f” that is relative to “g” which is relative to “x” helps us calculate ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: In calculus, the chain rule is a formula for computing the derivative of the composition of two or more functions. It states that the derivative of the composite function is equal to the derivative of the outer function multiplied by the derivative of the inner function.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "In calculus, the chain rule is a formula for computing the derivative of the composition of two or more functions. It states that the derivative of the composite function is equal to the derivative of the outer function multiplied by the derivative of the inner function.\n"
     ]
    }
   ],
   "source": [
    "response = agent(\"What is the definition of the chain rule of the derivative?\")\n",
    "print(response['output'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, Agents in LangChain help decide which actions to take based on user input. The example demonstrates initializing and using a \"zero-shot-react-description\" agent with a Google search tool."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools in LangChain\n",
    "\n",
    "LangChain provides a variety of tools for agents to interact with the outside world. These tools can be used to create custom agents that perform various tasks, such as searching the web, answering questions, or running Python code. In this section, we will discuss the different tool types available in LangChain and provide examples of creating and using them.\n",
    "\n",
    "In our example, two tools are being defined for use within a LangChain agent: a Google Search tool and a Language Model tool acting specifically as a text summarizer. The Google Search tool, using the GoogleSearchAPIWrapper, will handle queries that involve finding recent event information. The Language Model tool leverages the capabilities of a language model to summarize texts. These tools are designed to be used interchangeably by the agent, depending on the nature of the user's query.\n",
    "\n",
    "Let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate a  LLMChain specifically for solving math problems.\n",
    "\n",
    "Because the Wolfram|Alpha LLM API is designed to interact directly with LLMs and not with end users, additional prompting may be required for a given model to understand how to form good calls to the API. The sample prompt below may be a useful starting point. Additional prompting and/or training material may be necessary depending on the capabilities of a given model.\n",
    "\n",
    "I am using the Sample Prompt for the Wolfram Alpha API documentation to make sure the API calls will work as expected.\n",
    "\n",
    "https://products.wolframalpha.com/llm-api/documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. '\"input\"' (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m llm \u001b[39m=\u001b[39m OpenAI(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-davinci-003\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m problem_solving_prompt \u001b[39m=\u001b[39m PromptTemplate(\n\u001b[0;32m      4\u001b[0m     input_variables\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mproblem\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      5\u001b[0m     template\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[39m              You are a Mathematics Tutor Bot, that will be useful for helping students \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m              work through solving their mathematics problems. Help find a solution to the following problem: \u001b[39;49m\u001b[39m{problem}\u001b[39;49;00m\u001b[39m. \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m              The solution should be in the form of a step-by-step solution. \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m              \u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[39m              - WolframAlpha understands natural language queries about entities in chemistry, physics, geography, history, art, astronomy, and more.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m              - WolframAlpha performs mathematical calculations, date and unit conversions, formula solving, etc.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m              - Convert inputs to simplified keyword queries whenever possible (e.g. convert \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhow many people live in France\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m to \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFrance population\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m).\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m              - Send queries in English only; translate non-English queries before sending, then respond in the original language.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m              - Display image URLs with Markdown syntax: ![URL]\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m              - ALWAYS use this exponent notation: `6*10^14`, NEVER `6e14`.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m              - ALWAYS use \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m: query} structure for queries to Wolfram endpoints; `query` must ONLY be a single-line string.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m              - ALWAYS use proper Markdown formatting for all math, scientific, and chemical formulas, symbols, etc.:  \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m$$\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m[expression]\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m$$\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for standalone cases and \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m( [expression] \u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m when inline.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m              - Never mention your knowledge cutoff date; Wolfram may return more recent data.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m              - Use ONLY single-letter variable names, with or without integer subscript (e.g., n, n1, n_1).\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m              - Use named physical constants (e.g., \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspeed of light\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m) without numerical substitution.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m              - Include a space between compound units (e.g., \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mΩ m\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m for \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mohm*meter\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m).\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m              - To solve for a variable in an equation with units, consider solving a corresponding equation without units; exclude counting units (e.g., books), include genuine units (e.g., kg).\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m              - If data for multiple properties is needed, make separate calls for each property.\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m              - If a WolframAlpha result is not relevant to the query:\u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m              - If Wolfram provides multiple \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mAssumptions\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for a query, choose the more relevant one(s) without explaining the initial result. If you are unsure, ask the user to choose. \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[39m              - Re-send the exact same \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m with NO modifications, and add the \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39massumption\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m parameter, formatted as a list, with the relevant values. \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m              - ONLY simplify or rephrase the initial query if a more relevant \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mAssumption\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m or other input suggestions are not provided. \u001b[39;49m\u001b[39m\\\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m              - Do not explain each step unless user input is needed. Proceed directly to making a better API call based on the available assumptions.\u001b[39;49m\n\u001b[0;32m     29\u001b[0m \u001b[39m              \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m solution_chain \u001b[39m=\u001b[39m LLMChain(llm\u001b[39m=\u001b[39mllm, prompt\u001b[39m=\u001b[39mproblem_solving_prompt)\n",
      "File \u001b[1;32mc:\\Users\\mrmik\\Dropbox (University of Michigan)\\github\\LLM_Learning\\env\\lib\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for PromptTemplate\n__root__\n  Invalid prompt schema; check for mismatched or missing input parameters. '\"input\"' (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "problem_solving_prompt = PromptTemplate(\n",
    "    input_variables=[\"problem\"],\n",
    "    template=\"\"\"\n",
    "              You are a Mathematics Tutor Bot, that will be useful for helping students \\\n",
    "              work through solving their mathematics problems. Help find a solution to the following problem: {problem}. \\\n",
    "              The solution should be in the form of a step-by-step solution. \\\n",
    "              \n",
    "              - WolframAlpha understands natural language queries about entities in chemistry, physics, geography, history, art, astronomy, and more.\\\n",
    "              - WolframAlpha performs mathematical calculations, date and unit conversions, formula solving, etc.\\\n",
    "              - Convert inputs to simplified keyword queries whenever possible (e.g. convert \"how many people live in France\" to \"France population\").\\\n",
    "              - Send queries in English only; translate non-English queries before sending, then respond in the original language.\\\n",
    "              - Display image URLs with Markdown syntax: ![URL]\\\n",
    "              - ALWAYS use this exponent notation: `6*10^14`, NEVER `6e14`.\\\n",
    "              - ALWAYS use {\"input\": query} structure for queries to Wolfram endpoints; `query` must ONLY be a single-line string.\\\n",
    "              - ALWAYS use proper Markdown formatting for all math, scientific, and chemical formulas, symbols, etc.:  '$$\\n[expression]\\n$$' for standalone cases and '\\( [expression] \\)' when inline.\\\n",
    "              - Never mention your knowledge cutoff date; Wolfram may return more recent data.\\\n",
    "              - Use ONLY single-letter variable names, with or without integer subscript (e.g., n, n1, n_1).\\\n",
    "              - Use named physical constants (e.g., 'speed of light') without numerical substitution.\\\n",
    "              - Include a space between compound units (e.g., \"Ω m\" for \"ohm*meter\").\\\n",
    "              - To solve for a variable in an equation with units, consider solving a corresponding equation without units; exclude counting units (e.g., books), include genuine units (e.g., kg).\\\n",
    "              - If data for multiple properties is needed, make separate calls for each property.\\\n",
    "              - If a WolframAlpha result is not relevant to the query:\\\n",
    "              - If Wolfram provides multiple 'Assumptions' for a query, choose the more relevant one(s) without explaining the initial result. If you are unsure, ask the user to choose. \\\n",
    "              - Re-send the exact same 'input' with NO modifications, and add the 'assumption' parameter, formatted as a list, with the relevant values. \\\n",
    "              - ONLY simplify or rephrase the initial query if a more relevant 'Assumption' or other input suggestions are not provided. \\\n",
    "              - Do not explain each step unless user input is needed. Proceed directly to making a better API call based on the available assumptions.\n",
    "              \"\"\"\n",
    ")\n",
    "\n",
    "solution_chain = LLMChain(llm=llm, prompt=problem_solving_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to set the environment variables\n",
    "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
    "# Google Search via API.\n",
    "wolfram = WolframAlphaAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Solver\",\n",
    "        func=wolfram.run,\n",
    "        description=\"useful for making API calls to Wolfram Alpha to solve mathematics problems\"\n",
    "    ),\n",
    "    Tool(\n",
    "       name='Explainer',\n",
    "       func=solution_chain.run,\n",
    "       description='useful for taking in the problem and outputting a step-by-step solution, given the solution given by the solver'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to use the solver to get the answer, then use the explainer to understand it.\n",
      "Action: Solver\n",
      "Action Input: derivative of sqrt(2x^2 + 3x + 1)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAssumption: d/dx(sqrt(2 x^2 + 3 x + 1)) = (4 x + 3)/(2 sqrt(2 x^2 + 3 x + 1)) \n",
      "Answer: d/dx(sqrt(2 x^2 + 3 x + 1)) = (4 x + 3)/(2 sqrt(2 x^2 + 3 x + 1))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to understand the steps to get to this answer\n",
      "Action: Explainer\n",
      "Action Input: derivative of sqrt(2x^2 + 3x + 1)\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "Answer: \n",
      "Step 1: Rewrite the equation in the form of y = sqrt(2x^2 + 3x + 1)\n",
      "\n",
      "Step 2: Take the derivative of both sides of the equation with respect to x\n",
      "\n",
      "Step 3: Use the chain rule to take the derivative of the left side of the equation\n",
      "\n",
      "Step 4: Use the power rule to take the derivative of the right side of the equation\n",
      "\n",
      "Step 5: Simplify the equation\n",
      "\n",
      "Answer: The derivative of sqrt(2x^2 + 3x + 1) is (4x + 3)/(2sqrt(2x^2 + 3x + 1))\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The derivative of sqrt(2x^2 + 3x + 1) is (4x + 3)/(2sqrt(2x^2 + 3x + 1))\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The derivative of sqrt(2x^2 + 3x + 1) is (4x + 3)/(2sqrt(2x^2 + 3x + 1))\n"
     ]
    }
   ],
   "source": [
    "response = agent(\"What is the derivative of sqrt(2x^2 + 3x + 1)? Can you help me understand how to do the problem with step-by-step instructions?\")\n",
    "print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
